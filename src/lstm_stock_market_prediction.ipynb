{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlphaVantage API Key: 4RAX5IX44VPJZ708\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\users\\trobb\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import urllib\n",
    "import json\n",
    "import datetime as dt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data updated and saved.\n"
     ]
    }
   ],
   "source": [
    "# API key from Alpha Vantage\n",
    "api_key = '4RAX5IX44VPJZ708'\n",
    "\n",
    "# Ticker symbol for Amazon\n",
    "ticker = \"AMZN\"\n",
    "\n",
    "# Construct URL to fetch stock data\n",
    "url_string = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&outputsize=full&apikey={api_key}\"\n",
    "\n",
    "# CSV filename to save stock data\n",
    "file_to_save = f'stock_market_data-{ticker}.csv'\n",
    "\n",
    "\n",
    "try:\n",
    "    # Fetch new data from API\n",
    "    with urllib.request.urlopen(url_string) as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "        data = data['Time Series (Daily)']\n",
    "        new_df = pd.DataFrame(columns=['Date', 'Low', 'High', 'Close', 'Open'])\n",
    "        for k, v in data.items():\n",
    "            date = dt.datetime.strptime(k, '%Y-%m-%d')\n",
    "            data_row = [date.date(), float(v['3. low']), float(v['2. high']),\n",
    "                        float(v['4. close']), float(v['1. open'])]\n",
    "            new_df.loc[-1, :] = data_row\n",
    "            new_df.index = new_df.index + 1\n",
    "        new_df = new_df.sort_index()\n",
    "\n",
    "    # Load existing data if it exists\n",
    "    if os.path.exists(file_to_save):\n",
    "        existing_df = pd.read_csv(file_to_save)\n",
    "        existing_df['Date'] = pd.to_datetime(existing_df['Date'])\n",
    "        existing_df.set_index('Date', inplace=True)\n",
    "\n",
    "        # Combine new data with existing data\n",
    "        combined_df = pd.concat([existing_df, new_df]).drop_duplicates()\n",
    "        combined_df.reset_index(drop=True, inplace=True)\n",
    "    else:\n",
    "        combined_df = new_df\n",
    "\n",
    "    # Save updated data back to CSV\n",
    "    combined_df.to_csv(file_to_save, index=False)\n",
    "    print(\"Data updated and saved.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while updating data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Low, High, Close, Open, Date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure the index is of datetime type and sort it\n",
    "combined_df.index = pd.to_datetime(combined_df.index)\n",
    "combined_df.sort_index(inplace=True)\n",
    "\n",
    "# Now try filtering the data from 2020 onwards\n",
    "df_2020 = combined_df['2020-01-01':]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(df_2020.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correctly checking if the index is monotonic increasing\n",
    "if df.index.is_monotonic_increasing:\n",
    "    df_2020 = df['2020-01-01':]\n",
    "    print(\"Index is monotonic increasing. Data filtered successfully.\")\n",
    "    print(df_2020.head())\n",
    "else:\n",
    "    print(\"Index is not monotonic increasing. Sorting might be needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if the starting date exists in the index, if not, adjust\n",
    "start_date = '2020-01-01'\n",
    "if start_date in df.index:\n",
    "    df_2020 = df[start_date:]\n",
    "else:\n",
    "    # Find the next date after '2020-01-01' in the index\n",
    "    start_date = df[df.index >= start_date].index.min()\n",
    "    df_2020 = df[start_date:]\n",
    "\n",
    "print(df_2020.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Assume df is your DataFrame and it includes a 'Close' price column\n",
    "close_prices = df['Close'].values.reshape(-1, 1)  # Reshape for scaling\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale the 'Close' prices\n",
    "scaled_close = scaler.fit_transform(close_prices)\n",
    "\n",
    "# Now you can safely call the function\n",
    "sequence_length = 60\n",
    "X, y = create_sequences(scaled_close, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences of data\n",
    "def create_sequences(data, sequence_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(sequence_length, len(data)):\n",
    "        X.append(data[i-sequence_length:i, 0])\n",
    "        y.append(data[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences with a window of the last 60 days\n",
    "sequence_length = 60\n",
    "X, y = create_sequences(scaled_close, sequence_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# Assuming 'model' is already defined\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    LSTM(50),\n",
    "    Dense(1)  # Predicting a single continuous value\n",
    "])\n",
    "\n",
    "# Compile the model with a suitable loss function for regression\n",
    "model.compile(optimizer='adam', loss=MeanSquaredError(), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, fit the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = model.predict(X_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "y_test_scaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After making predictions\n",
    "# ... [Code to make predictions] ...\n",
    "\n",
    "# Get the last `len(predictions)` dates from the DataFrame for plotting\n",
    "plot_dates = df.index[-len(predictions):]\n",
    "\n",
    "# Plotting actual vs. predicted prices\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(plot_dates, y_test_scaled, color='blue', label='Actual AMZN Close Price')\n",
    "plt.plot(plot_dates, predictions, color='red', linestyle='dashed', label='Predicted AMZN Close Price')\n",
    "plt.title('AMZN Stock Price Prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('AMZN Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'predictions' and 'y_test_scaled' are the predicted and actual prices from the test set\n",
    "\n",
    "# Calculate MAE, MSE, RMSE, and R2\n",
    "mae = mean_absolute_error(y_test_scaled, predictions)\n",
    "mse = mean_squared_error(y_test_scaled, predictions)\n",
    "rmse = np.sqrt(mse)  # Or directly use mean_squared_error(..., squared=False)\n",
    "r2 = r2_score(y_test_scaled, predictions)\n",
    "\n",
    "# Print out the metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
